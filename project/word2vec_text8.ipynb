{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = word2vec.Text8Corpus('/home/sam/Hhd/data/text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Text8Corpus at 0x7f88b4457d10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-20 22:00:38,264 : INFO : collecting all words and their counts\n",
      "2017-02-20 22:00:38,266 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-02-20 22:00:41,815 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-02-20 22:00:41,815 : INFO : Loading a fresh vocabulary\n",
      "2017-02-20 22:00:42,027 : INFO : min_count=5 retains 71290 unique words (28% of original 253854, drops 182564)\n",
      "2017-02-20 22:00:42,028 : INFO : min_count=5 leaves 16718844 word corpus (98% of original 17005207, drops 286363)\n",
      "2017-02-20 22:00:42,132 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-02-20 22:00:42,155 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-02-20 22:00:42,156 : INFO : downsampling leaves estimated 12506280 word corpus (74.8% of prior 16718844)\n",
      "2017-02-20 22:00:42,156 : INFO : estimated required memory for 71290 words and 100 dimensions: 92677000 bytes\n",
      "2017-02-20 22:00:42,296 : INFO : resetting layer weights\n",
      "2017-02-20 22:00:42,728 : INFO : training model with 4 workers on 71290 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-02-20 22:00:42,728 : INFO : expecting 1701 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-20 22:00:43,737 : INFO : PROGRESS: at 2.16% examples, 1340791 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-20 22:00:44,743 : INFO : PROGRESS: at 4.51% examples, 1398151 words/s, in_qsize 8, out_qsize 0\n",
      "2017-02-20 22:00:45,749 : INFO : PROGRESS: at 6.88% examples, 1428036 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-20 22:00:46,751 : INFO : PROGRESS: at 9.12% examples, 1422572 words/s, in_qsize 8, out_qsize 1\n",
      "2017-02-20 22:00:47,753 : INFO : PROGRESS: at 11.31% examples, 1412686 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-20 22:00:48,759 : INFO : PROGRESS: at 13.52% examples, 1406567 words/s, in_qsize 7, out_qsize 1\n",
      "2017-02-20 22:00:49,761 : INFO : PROGRESS: at 15.68% examples, 1396722 words/s, in_qsize 7, out_qsize 1\n",
      "2017-02-20 22:00:50,766 : INFO : PROGRESS: at 17.90% examples, 1393961 words/s, in_qsize 8, out_qsize 0\n",
      "2017-02-20 22:00:51,769 : INFO : PROGRESS: at 20.08% examples, 1389617 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-20 22:00:52,771 : INFO : PROGRESS: at 22.38% examples, 1392555 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-20 22:00:53,775 : INFO : PROGRESS: at 24.73% examples, 1399387 words/s, in_qsize 8, out_qsize 0\n",
      "2017-02-20 22:00:54,777 : INFO : PROGRESS: at 27.02% examples, 1403315 words/s, in_qsize 4, out_qsize 1\n",
      "2017-02-20 22:00:55,785 : INFO : PROGRESS: at 29.21% examples, 1400108 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-20 22:00:56,791 : INFO : PROGRESS: at 31.56% examples, 1405035 words/s, in_qsize 8, out_qsize 0\n",
      "2017-02-20 22:00:57,796 : INFO : PROGRESS: at 33.82% examples, 1405290 words/s, in_qsize 6, out_qsize 1\n",
      "2017-02-20 22:00:58,799 : INFO : PROGRESS: at 36.08% examples, 1404922 words/s, in_qsize 7, out_qsize 1\n",
      "2017-02-20 22:00:59,808 : INFO : PROGRESS: at 38.25% examples, 1401007 words/s, in_qsize 7, out_qsize 1\n",
      "2017-02-20 22:01:00,817 : INFO : PROGRESS: at 40.49% examples, 1400188 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-20 22:01:01,824 : INFO : PROGRESS: at 42.69% examples, 1397686 words/s, in_qsize 5, out_qsize 2\n",
      "2017-02-20 22:01:02,829 : INFO : PROGRESS: at 44.84% examples, 1394950 words/s, in_qsize 7, out_qsize 1\n",
      "2017-02-20 22:01:03,832 : INFO : PROGRESS: at 46.96% examples, 1392058 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-20 22:01:04,837 : INFO : PROGRESS: at 49.10% examples, 1389540 words/s, in_qsize 3, out_qsize 1\n",
      "2017-02-20 22:01:05,842 : INFO : PROGRESS: at 51.24% examples, 1387426 words/s, in_qsize 4, out_qsize 2\n",
      "2017-02-20 22:01:06,847 : INFO : PROGRESS: at 53.46% examples, 1387236 words/s, in_qsize 6, out_qsize 1\n",
      "2017-02-20 22:01:07,851 : INFO : PROGRESS: at 55.78% examples, 1389127 words/s, in_qsize 7, out_qsize 2\n",
      "2017-02-20 22:01:08,867 : INFO : PROGRESS: at 57.97% examples, 1387611 words/s, in_qsize 5, out_qsize 3\n",
      "2017-02-20 22:01:09,862 : INFO : PROGRESS: at 60.38% examples, 1391645 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-20 22:01:10,868 : INFO : PROGRESS: at 62.59% examples, 1390578 words/s, in_qsize 6, out_qsize 2\n",
      "2017-02-20 22:01:11,878 : INFO : PROGRESS: at 64.91% examples, 1392454 words/s, in_qsize 7, out_qsize 1\n",
      "2017-02-20 22:01:12,880 : INFO : PROGRESS: at 67.13% examples, 1392549 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-20 22:01:13,881 : INFO : PROGRESS: at 69.32% examples, 1392085 words/s, in_qsize 8, out_qsize 1\n",
      "2017-02-20 22:01:14,888 : INFO : PROGRESS: at 71.60% examples, 1393259 words/s, in_qsize 8, out_qsize 0\n",
      "2017-02-20 22:01:15,884 : INFO : PROGRESS: at 73.96% examples, 1395685 words/s, in_qsize 8, out_qsize 1\n",
      "2017-02-20 22:01:16,891 : INFO : PROGRESS: at 76.17% examples, 1394798 words/s, in_qsize 5, out_qsize 1\n",
      "2017-02-20 22:01:17,903 : INFO : PROGRESS: at 78.41% examples, 1394512 words/s, in_qsize 6, out_qsize 3\n",
      "2017-02-20 22:01:18,905 : INFO : PROGRESS: at 80.63% examples, 1394139 words/s, in_qsize 6, out_qsize 1\n",
      "2017-02-20 22:01:19,904 : INFO : PROGRESS: at 82.95% examples, 1395082 words/s, in_qsize 8, out_qsize 1\n",
      "2017-02-20 22:01:20,907 : INFO : PROGRESS: at 85.19% examples, 1395212 words/s, in_qsize 8, out_qsize 0\n",
      "2017-02-20 22:01:21,912 : INFO : PROGRESS: at 87.56% examples, 1397726 words/s, in_qsize 8, out_qsize 0\n",
      "2017-02-20 22:01:22,921 : INFO : PROGRESS: at 89.78% examples, 1397320 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-20 22:01:23,922 : INFO : PROGRESS: at 91.99% examples, 1397087 words/s, in_qsize 8, out_qsize 0\n",
      "2017-02-20 22:01:24,927 : INFO : PROGRESS: at 94.31% examples, 1398380 words/s, in_qsize 7, out_qsize 1\n",
      "2017-02-20 22:01:25,927 : INFO : PROGRESS: at 96.48% examples, 1397044 words/s, in_qsize 8, out_qsize 0\n",
      "2017-02-20 22:01:26,926 : INFO : PROGRESS: at 98.61% examples, 1395447 words/s, in_qsize 3, out_qsize 0\n",
      "2017-02-20 22:01:27,526 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-02-20 22:01:27,529 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-20 22:01:27,531 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-20 22:01:27,535 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-20 22:01:27,535 : INFO : training on 85026035 raw words (62532186 effective words) took 44.8s, 1395687 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(sentences, size=100, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-20 22:01:46,608 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'queen', 0.676036536693573)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'queen', 0.676036536693573), (u'empress', 0.6634204387664795)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 0.7367984652519226),\n",
       " (u'girl', 0.6720128655433655),\n",
       " (u'creature', 0.6221809387207031),\n",
       " (u'stranger', 0.5950769782066345),\n",
       " (u'bride', 0.5916381478309631),\n",
       " (u'demon', 0.5718533992767334),\n",
       " (u'boy', 0.5707845687866211),\n",
       " (u'evil', 0.562491238117218),\n",
       " (u'god', 0.5518344640731812),\n",
       " (u'soul', 0.551337480545044)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-20 22:01:54,402 : INFO : saving Word2Vec object under text8.model, separately None\n",
      "2017-02-20 22:01:54,403 : INFO : not storing attribute syn0norm\n",
      "2017-02-20 22:01:54,404 : INFO : not storing attribute cum_table\n",
      "2017-02-20 22:01:54,870 : INFO : saved text8.model\n"
     ]
    }
   ],
   "source": [
    "model.save('text8.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-20 22:01:56,766 : INFO : storing 71290x100 projection weights into text.model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_word2vec_format('text.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-20 22:01:57,879 : INFO : loading projection weights from text.model.bin\n",
      "2017-02-20 22:01:58,365 : INFO : loaded (71290, 100) matrix from text.model.bin\n"
     ]
    }
   ],
   "source": [
    "model1 = word2vec.Word2Vec.load_word2vec_format('text.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-20 22:01:58,506 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'mother', 0.7939096689224243),\n",
       " (u'grandmother', 0.7350692749023438),\n",
       " (u'wife', 0.7336280941963196)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.most_similar(['girl', 'father'], ['boy'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'he' is to 'is' as 'she' is to 'was'\n",
      "'big' is to 'bigger' as 'bad' is to 'worse'\n",
      "'going' is to 'went' as 'being' is to 'was'\n"
     ]
    }
   ],
   "source": [
    "more_examples = [\"he is she\", \"big bigger bad\", \"going went being\"]\n",
    "for example in more_examples:\n",
    "    a, b, x = example.split()\n",
    "    predicted = model.most_similar([x, b], [a])[0][0]\n",
    "    print \"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model_org = word2vec.Word2Vec.load_word2vec_format('vectors.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73679842105780868"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.08871603,  0.79077089, -2.29177451,  1.7176106 , -2.16895795,\n",
       "        0.09834326,  0.41825402, -0.46378225, -0.02012794,  0.32455352,\n",
       "       -1.038571  , -1.66496086, -0.12987994, -3.13930082,  3.29934669,\n",
       "       -3.80790377,  2.41025853, -1.59926343, -1.27460241,  1.76456583,\n",
       "        3.77646375, -1.7840296 ,  2.11935353, -1.14899659,  0.87994498,\n",
       "        0.35554257, -1.95739388,  1.90232527, -3.5156765 , -0.42760342,\n",
       "       -1.82414806,  0.27888811,  0.58191764,  0.1641009 ,  1.03966761,\n",
       "       -2.52580166,  1.87010312, -3.11309385, -1.64437068, -1.07024467,\n",
       "       -0.34432939, -0.96032888, -0.78158736, -2.20927501,  1.10502708,\n",
       "       -1.84310663,  1.73713267,  1.32420027, -0.13082792,  0.29546285,\n",
       "       -1.44650209, -0.34878954, -1.18852007,  0.16610546,  2.08293176,\n",
       "        0.34691527, -0.62345964,  0.14977133,  0.9905082 , -1.77908862,\n",
       "       -1.86338139, -0.17648426, -1.01571798,  0.6042282 , -1.79418111,\n",
       "       -1.86645114,  0.44147202,  1.258587  ,  0.55393767,  2.60430217,\n",
       "       -0.56793106, -2.53351951,  2.14856672, -1.35004056,  1.05281293,\n",
       "       -0.23379214,  1.46892285, -0.48352405, -3.0852735 ,  0.44860956,\n",
       "       -0.62700385, -0.09289996, -3.08005095,  1.39879179, -0.53232992,\n",
       "        1.17008376,  1.30913198, -1.0237999 , -1.21876895,  1.57701588,\n",
       "       -3.91836834,  1.16385472,  2.50986385, -0.31868845,  0.66997546,\n",
       "        1.20734274, -1.67674315, -0.03395709, -0.40179169, -0.65465957], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print model['computer'].shape\n",
    "model['computer']  # raw numpy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
